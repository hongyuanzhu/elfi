{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Assuming we are in the notebook directory add this so that we can import the library\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with ELFI: the MA(2) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2nd order moving average model, MA(2), is a common model used in univariate time analysis. Assuming zero mean it can be written as\n",
    "\n",
    "$$\n",
    "y_t = w_t + \\theta_1 w_{t-1} + \\theta_2 w_{t-2},\n",
    "$$\n",
    "\n",
    "where $\\theta_1, \\theta_2 \\in \\mathbb{R}$ and $(w_k)_{k\\in \\mathbb{Z}} \\sim N(0,1)$ represents an independent and identically distributed sequence of white noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The observed data and the inference problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, our task is to infer the parameters $\\theta_1, \\theta_2$ given a sequence of 100 observations $y$ that originate from an MA(2) process. Let's define this MA(2) simulator as a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MA2(t1, t2, batch_size=1, random_state=None):\n",
    "    n_obs = 100\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState()\n",
    "    w = random_state.randn(batch_size, n_obs+2) # i.i.d. sequence ~ N(0,1)\n",
    "    y = w[:,2:] + t1 * w[:,1:-1] + t2 * w[:,:-2]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ELFI, all non-constant data is in at least 2-dimensional NumPy arrays with different observations on the 0-axis. **Important**: in order to guarantee a consistent state of pseudo-random number generation, the simulator must have `random_state` as a keyword argument for reading in a `numpy.RandomState` object. Additionally, a simulator that supports vectorized operations should accept the keyword argument `batch_size` that defines how many sequences should be returned.\n",
    "\n",
    "Let's now use this simulator to create the observations with true parameter values $\\theta_1=0.6, \\theta_2=0.2$ (from now on these are considered unknown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# true parameters\n",
    "t1_0 = 0.6\n",
    "t2_0 = 0.2\n",
    "\n",
    "# Set up observed data y with some random seed\n",
    "random_state = np.random.RandomState(20161130)\n",
    "y = MA2(t1_0, t2_0, random_state=random_state)\n",
    "\n",
    "# Plot the observed sequence\n",
    "plt.figure(figsize=(11, 6));\n",
    "plt.plot(y.flatten());\n",
    "\n",
    "# To illustrate the stochasticity, let's plot a couple of more observations with the same true parameters:\n",
    "plt.plot(MA2(t1_0, t2_0).flatten());\n",
    "plt.plot(MA2(t1_0, t2_0).flatten());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Bayesian Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above illustrates the difficulty of inferring $\\theta_1, \\theta_2$. One way to approach this kind of problems is Approximate Bayesian Computation (ABC), which is based on the intuition that similar data is likely to have been produced by similar parameters. Although the idea may appear inapplicable for the task at hand, it works when a large number of samples can be used. For more information about ABC, please see e.g. \n",
    "\n",
    "* [Marin, J.-M., Pudlo, P., Robert, C. P., and Ryder, R. J. (2012). Approximate Bayesian computational\n",
    "methods. *Statistics and Computing*, 22(6):1167â€“1180.](http://link.springer.com/article/10.1007/s11222-011-9288-2)\n",
    "* [Lintusaari, J., Gutmann, M. U., Dutta, R., Kaski, S., and Corander, J. (2016). Fundamentals and recent\n",
    "developments in approximate Bayesian computation. *Systematic Biology*, doi: 10.1093/sysbio/syw077.](http://sysbio.oxfordjournals.org/content/early/2016/09/07/sysbio.syw077.full.pdf)\n",
    "* https://en.wikipedia.org/wiki/Approximate_Bayesian_computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the inference problem in ELFI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ELFI, the inference problem is described in the form of a directed acyclic graph ([DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph)), which associates each node with its parent nodes. This provides an intuitive means to describe complex dependencies that are automatically fulfilled by the inference engine. So let's build such a model.\n",
    "\n",
    "As is usual in Bayesian statistical inference, we need to define *prior* distributions for the unknown parameters $\\theta_1, \\theta_2$. In ELFI the priors can be any of the continuous and discrete distributions available in `scipy.stats` (for custom priors, see [below](#custom_prior)). For simplicity, let's start by assuming that both parameters follow `Uniform(0, 2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import elfi\n",
    "\n",
    "# a node is defined by giving it a name, a distribution from scipy.stats and its parents (here constants 0 and 2)\n",
    "t1 = elfi.Prior('t1', scipy.stats.uniform, 0, 2)\n",
    "\n",
    "# ELFI also supports giving the scipy.stats distributions as strings\n",
    "t2 = elfi.Prior('t2', 'uniform', 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the *Simulator* node by giving it the `MA2` function, and the priors as its parents. As this node can be compared with observations, we give them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = elfi.Simulator('MA2', MA2, t1, t2, observed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how does one compare the simulations with the observed sequence? As was evident from the plot of just a few observed sequences, a direct pointwise comparison is unproductive. Indeed, the comparison of simulated sequences is often the most difficult (and arbitrary) part of ABC. Typically one chooses one or more *ad hoc* summary statistics and then calculates the discrepancy between those.\n",
    "\n",
    "Here, we will apply the intuition arising from the definition of the MA(2) process, and use the autocovariances with lags 1 and 2 as the summary statistics, and evaluate the discrepancy with the common Euclidean L2-distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autocov(x, lag=1):\n",
    "    mu = np.mean(x, axis=1, keepdims=True)\n",
    "    C = np.mean(x[:,lag:] * x[:,:-lag], axis=1, keepdims=True) - mu**2.\n",
    "    return C\n",
    "\n",
    "def distance(x, y):\n",
    "    d = np.linalg.norm( np.array(x) - np.array(y), ord=2, axis=0)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is familiar by now, a `Summary` node is defined by giving the autocovariance function and the simulated data (which includes the observed as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S1 = elfi.Summary('S1', autocov, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The node syntax requires a function, so defining the second summary statistic as autocovariance with lag 2 is a bit more involved. Instead a writing a complete function from scratch, we can create a *closure* that fixes the keyword argument `lag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# this is a more efficient way to say `lambda x: autocov(x, lag=2)`\n",
    "autocov2 = partial(autocov, lag=2)\n",
    "\n",
    "S2 = elfi.Summary('S2', autocov2, Y)\n",
    "\n",
    "# Finish the model with the final node that calculates the squared distance (S1_sim-S1_obs)**2 + (S2_sim-S2_obs)**2\n",
    "d = elfi.Discrepancy('d', distance, S1, S2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the inference model is defined, ELFI can visualize the DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elfi.draw_model(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='custom_prior'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining custom priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the above definition is perfectly valid, having both prior distributions as `Uniform(0,2)` is bad, as the case becomes unidentifiable (i.e. the solution is not unique). To avoid this, *Marin et al. (2012)* defined the priors such that $-2<\\theta_1<2$ with $\\theta_1+\\theta_2>-1$ and $\\theta_1-\\theta_2<1$ i.e. the parameters are sampled from a triangle (see below).\n",
    "\n",
    "In ELFI, custom distributions can be defined as **static** classes (i.e. they only have static methods) that behave in a way similar to distributions from scipy.stats. To be safe they can inherit `elfi.Distribution`. In this case we only need these for sampling, so implementing the static `rvs` method suffices. As was in the context of simulators, it is important to accept the keyword argument `random_state`, which is needed for ELFI's internal book-keeping of pseudo-random number generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define prior for t1 as in Marin et al., 2012 with t1 in range [-b, b]\n",
    "class MarinPrior_t1(elfi.Distribution):\n",
    "    def rvs(b, size=1, random_state=None):\n",
    "        u = scipy.stats.uniform.rvs(loc=0, scale=1, size=size, random_state=random_state)\n",
    "        t1 = np.where(u<0.5, np.sqrt(2.*u)*b-b, -np.sqrt(2.*(1.-u))*b+b)\n",
    "        return t1\n",
    "\n",
    "# define prior for t2 conditionally on t1 as in Marin et al., 2012, in range [-a, a]\n",
    "class MarinPrior_t2(elfi.Distribution):\n",
    "    def rvs(t1, a, size=1, random_state=None):\n",
    "        locs = np.maximum(-a-t1, t1-a)\n",
    "        scales = a - locs\n",
    "        t2 = scipy.stats.uniform.rvs(loc=locs, scale=scales, size=size, random_state=random_state)\n",
    "        return t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indeed sample from a triangle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1_1000 = MarinPrior_t1.rvs(2, 1000)\n",
    "t2_1000 = MarinPrior_t2.rvs(t1_1000, 1, 1000)\n",
    "plt.scatter(t1_1000, t2_1000, s=4, edgecolor='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the earlier priors to the new ones in the inference model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Redefine the priors, since t1 and t2 already exist\n",
    "t1.redefine(MarinPrior_t1, 2)\n",
    "t2.redefine(MarinPrior_t2, t1, 1)\n",
    "\n",
    "elfi.draw_model(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic inference with rejection sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest ABC algorithm samples parameters from their prior distributions, runs the simulator with these and compares to the observations. The samples are either accepted or rejected depending on how large the discrepancy is. The accepted samples represent samples from the posterior distribution.\n",
    "\n",
    "In ELFI, ABC methods are initialized with the discrepancy node and a list of inferred parameters (in Python lists are defined with square brackets). The optional keyword argument `batch_size` is related to parallelization (see the chapter on [parallelization](#parallelization) below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rej = elfi.Rejection(d, [t1, t2], batch_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the ABC method has been initialized, samples can be drawn from it. By default, rejection sampling in ELFI works in `quantile` mode i.e. a certain quantile of the samples with smallest discrepancies is accepted. The `sample` method requires the number of output samples as a parameter. Note that the simulator is then run `(N/quantile)` times.\n",
    "\n",
    "The `sample` method returns a `Result` object, which contains several attributes and methods. For example the attribute 'samples' contains an OrderedDict of the posterior numpy arrays. For rejection sampling, other attributes include e.g. the 'threshold', which is the threshold value resulting in the requested quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Time and run the simulator\n",
    "N = 10000\n",
    "%time result = rej.sample(N, quantile=0.01)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rejection sampling can also be performed `threshold` based i.e. accepting all samples that result in a discrepancy below certain threshold. Note that since we require a fixed number of samples, there is no guarantee how many times the simulator will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time result2 = rej.sample(N, threshold=0.2)\n",
    "\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rejection sampler also has a method for reconsidering existing simulations with a potentially new threshold. By default it uses all existing simulations, but one may also choose to use more or less. With the `reject` method the threshold is fixed as is the number of simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time result3 = rej.reject(threshold=0.3)\n",
    "\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances of `Result` contain methods for some basic plotting (these are convenience methods to plotting functions defined in `elfi.visualization`).\n",
    "\n",
    "For example one can plot the marginal distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.plot_marginals();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often \"pairwise relationships\" are more informative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.plot_pairs();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, ELFI uses the [Dask](http://dask.pydata.org/) library to automatically parallelize (most of) the computational inference. The parallelization can be tuned with the keyword argument `batch_size` for the Rejection class, which tells how many \"runs\"$^1$ should be sent to each available computational unit at a time. There is some overhead involved in the parallelization, so batches should be large, but not too large to eat all your memory.\n",
    "\n",
    "$^1$To be more accurate, the MA2 model as well as the other functions in this example are vectorized, and the simulator is actually called just a few times with the keyword argument `n_sim` set to `batch_size`. This results in more efficient usage of numpy operations. Whenever possible, functions should be written in a form that allows vectorization.\n",
    "\n",
    "The underlying Dask graph for our inference can be visualized (partly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask.dot import dot_graph\n",
    "dot_graph(d[0:40000].dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Monte Carlo ABC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rejection sampling is quite inefficient, as it does not learn from its history. The sequential Monte Carlo (SMC) ABC algorithm does just that by applying importance sampling: samples are *weighed* according to the resulting discrepancies and the next *population* of samples is drawn near to the previous using the weights as probabilities. \n",
    "\n",
    "For evaluating the weights, SMC ABC needs to have probability density functions for the priors. In our MA2 example the second prior is conditional on the first, which complicates matters a bit. Let's modify the prior distribution classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define prior for t1 as in Marin et al., 2012 with t1 in range [-b, b]\n",
    "class MarinPrior_t1(elfi.Distribution):\n",
    "    def rvs(b, size=1, random_state=None):\n",
    "        u = scipy.stats.uniform.rvs(loc=0, scale=1, size=size, random_state=random_state)\n",
    "        t1 = np.where(u<0.5, np.sqrt(2.*u)*b-b, -np.sqrt(2.*(1.-u))*b+b)\n",
    "        return t1\n",
    "    \n",
    "    def pdf(x, b):\n",
    "        p = 1./b - np.abs(x) / (b*b)\n",
    "        p = np.where(p < 0., 0., p)  # disallow values outside of [-b, b] (affects weights only)\n",
    "        return p\n",
    "\n",
    "# define prior for t2 conditionally on t1 as in Marin et al., 2012, in range [-a, a]\n",
    "class MarinPrior_t2(elfi.Distribution):\n",
    "    def rvs(t1, a, size=1, random_state=None):\n",
    "        locs = np.maximum(-a-t1, t1-a)\n",
    "        scales = a - locs\n",
    "        t2 = scipy.stats.uniform.rvs(loc=locs, scale=scales, size=size, random_state=random_state)\n",
    "        return t2\n",
    "    \n",
    "    def pdf(x, t1, a):\n",
    "        locs = np.maximum(-a-t1, t1-a)\n",
    "        scales = a - locs\n",
    "        p = scipy.stats.uniform.pdf(x, loc=locs, scale=scales)\n",
    "        p = np.where(scales>0., p, 0.)  # disallow values outside of [-a, a] (affects weights only)\n",
    "        return p\n",
    "    \n",
    "# Redefine the priors\n",
    "t1.redefine(MarinPrior_t1, 2)\n",
    "t2.redefine(MarinPrior_t2, t1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ELFI, one can setup a SMC ABC sampler just like the Rejection sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smc = elfi.SMC(d, [t1, t2], batch_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sampling, one has to define the number of output samples, the number of populations and a *schedule* i.e. a list of quantiles to use for each population. In essence, a population is just refined rejection sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "n_populations = 3\n",
    "schedule = [0.7, 0.2, 0.05]\n",
    "%time result_dict = smc.sample(N, n_populations, schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=n_populations, sharex=True, sharey=True, figsize=(16,6))\n",
    "\n",
    "samples = result_dict['samples_history'] + [result_dict['samples']]\n",
    "weights = result_dict['weights_history'] + [result_dict['weights']]\n",
    "\n",
    "for ii in range(n_populations):\n",
    "    s = samples[ii]\n",
    "    w = weights[ii]\n",
    "    print(\"Posterior means for population {}: t1: {:.2f} t2: {:.2f}\".format(ii+1,\\\n",
    "                                                                            np.average(s[0], weights=w, axis=0)[0],\\\n",
    "                                                                            np.average(s[1], weights=w, axis=0)[0]))\n",
    "    ax[ii].scatter(s[0], s[1], s=1, edgecolor='none');\n",
    "    ax[ii].set_title(\"Population {}\".format(ii+1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the populations iteratively concentrate more and more around the true parameter values.\n",
    "\n",
    "Note that for the later populations some of the samples lie outside allowed region. This is due to the SMC algorithm sampling near previous samples, with *near* meaning a Gaussian distribution centered around previous samples with variance as twice the weighted empirical variance. However, the outliers carry zero weight, as was defined in the prior pdfs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization for Likelihood-Free Inference (BOLFI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice inference problems often have a more complicated and computationally heavy simulator than the function `MA2` here, and one simply cannot run it for millions of times. The [BOLFI](https://arxiv.org/abs/1501.03291) framework is likely to prove useful in such situation: a statistical model (e.g. [Gaussian process](https://en.wikipedia.org/wiki/Gaussian_process)) is created for the discrepancy, and its minimum is inferred with [Bayesian optimization](https://en.wikipedia.org/wiki/Bayesian_optimization). This approach typically reduces the number of required simulator calls by several orders of magnitude.\n",
    "\n",
    "As BOLFI is more advanced inference method, its interface is also a bit more involved. But not much: Using the same graphical model as earlier, the inference begins by defining a Gaussian process model, for which we use the [GPy](https://sheffieldml.github.io/GPy/) library. We are inferring 2 parameters with the same bounds as earlier, and we have to give these conditions to the surrogate model i.e. the Gaussian process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gp_model = elfi.GPyModel(input_dim=2, bounds=((-3,3), (-3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# redefine the model\n",
    "elfi.env.new_inference_task()\n",
    "t1 = elfi.Prior('t1', scipy.stats.uniform, 0, 2)\n",
    "t2 = elfi.Prior('t2', 'uniform', 0, 2)\n",
    "Y = elfi.Simulator('MA2', MA2, t1, t2, observed=y)\n",
    "S1 = elfi.Summary('S1', autocov, Y)\n",
    "S2 = elfi.Summary('S2', autocov2, Y)\n",
    "d = elfi.Discrepancy('d', distance, S1, S2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a logarithm of the discrepancies reduces the effect that high discrepancies have on the GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_distance = lambda s1,s2: np.log(distance(s1, s2))\n",
    "d = d.change_to(elfi.Discrepancy('log_d', log_distance, S1, S2), transfer_parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the surrogate model, we can instantiate the BOLFI framework object in a somewhat similar way as earlier, except that we now additionally define the surrogate model and the number of samples to take from it (this is the number of simulator calls): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bolfi = elfi.BOLFI(d, [t1, t2], batch_size=5, n_surrogate_samples=150, model=gp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BOLFI class can now try to `infer` the posterior distribution of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post = bolfi.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gp_model.gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get estimates for *maximum a posteriori* and *maximum likelihood* easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post.MAP, post.ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrepancies for samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(16,6), sharey=True)\n",
    "ax[0].scatter(bolfi.model.gp.X[:,0], bolfi.model.gp.Y[:,0]);\n",
    "ax[1].scatter(bolfi.model.gp.X[:,1], bolfi.model.gp.Y[:,0]);\n",
    "ax[0].set_ylabel('Discrepancy');\n",
    "ax[0].set_xlabel('t1');\n",
    "ax[1].set_xlabel('t2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the Gaussian process using GPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bolfi.model.gp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or we can visualize the posterior directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = np.meshgrid(np.linspace(*bolfi.model.bounds[0]), np.linspace(*bolfi.model.bounds[1]))\n",
    "z = np.empty_like(x)\n",
    "for ii in range(len(x)):\n",
    "    for jj in range(len(x)):\n",
    "        z[ii, jj] = post.pdf(np.array([x[ii, jj], y[ii, jj]]))\n",
    "plt.contour(x, y, z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving samples for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section is still under construction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving performed computations is often a good idea, especially if the simulator is expensive to call. ELFI supports persistence of results in multiple formats. The user simply has to define this with the `store` keyword argument, which can be given to all nodes in the graphical model upon instantiation.\n",
    "\n",
    "To cache the prior `t1` in memory (affects only `t1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# t1 already exists\n",
    "elfi.env.new_inference_task()\n",
    "t1 = elfi.Prior('t1', 'uniform', 0, 1, store=\"cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store directly in a Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_samples = 10000000\n",
    "t1_array = np.empty((max_samples, 1))  # make sure you allocate enough memory\n",
    "elfi.env.new_inference_task()\n",
    "t1 = elfi.Prior('t1', 'uniform', 0, 1, store=t1_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A NoSQL database is also supported, in which case the results are saved to collection matching node name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nosql_store = elfi.UnQLiteStore()  # accepts filename to save to disk\n",
    "elfi.env.new_inference_task()\n",
    "Y = elfi.Simulator('MA2', MA2, t1, t2, observed=y, store=nosql_store)\n",
    "\n",
    "# afterwards get data:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
